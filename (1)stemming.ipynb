{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stemming.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ondYyT-tXQQS",
        "outputId": "35a0be44-5866-46b8-e938-9746e286cbd2"
      },
      "source": [
        "!pip install nltk\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVADNrEJczyh"
      },
      "source": [
        "## steming \n",
        "The process of reducing the infected words into stem word (best word)\n",
        "\n",
        "## StopWord \n",
        "these are the words which are less valuable so these words are dropping from paragraph "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBZ66pCDXY30",
        "outputId": "05f68a79-76c1-4f47-ef00-1ee48b5dc9b0"
      },
      "source": [
        "import nltk \n",
        "nltk.download('punkt')\n",
        "nltk.download(\"stopwords\") \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mkfgK-9dx3Q"
      },
      "source": [
        "## Defining the paragraph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sXE7txhbmVC"
      },
      "source": [
        "paragraph = \"\"\" The tweet without stop words and punctuation looks like this. However, note that in some contexts you won't have to eliminate punctuation. So you should think carefully about whether punctuation adds important\n",
        " information to your specific NLP task or not. Tweets and other types of texts often have handles and URLs, but these don't add any value for the task of sentiment analysis. Let's eliminate these two handles and this URL.\n",
        "  At the end of this process, the resulting tweets contains all the important information related to its sentiment. Tuning GREAT AI model is clearly a positive tweet and a sufficiently good model should be able to classify it.\n",
        "   Now that the tweet from the example has only the necessary information, I will perform stemming for every word. Stemming in NLP is simply transforming any word to its base stem, \n",
        "   which you could define as the set of characters that are used to construct the word and its derivatives. Let's take the first word from the example. Its stem is done, because adding the letter e, \n",
        "   it forms the word tune. Adding the suffix ed, forms the word tuned, and adding the suffix ing, it forms the word tuning. After you perform stemming on your corpus, \n",
        "   the word tune, tuned, and tuning will be reduced to the stem tun. So your vocabulary would be significantly reduced when you perform this process for every word in the corpus.\n",
        "    To reduce your vocabulary even further without losing valuable information, you'd have to lowercase every one of your words. So the word GREAT, Great and great would be treated as the same exact word. \n",
        "    This is the final preprocess tweet as a list of words. Now that you're familiar with stemming and stop words, you know the basics of texts processing. \n",
        "In the next video, you can use your process suite function to extract a matrix x, which represents all the tweets in your data set.\"\"\"                                                                                                                                                                                                              "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPFAH4OWef0G"
      },
      "source": [
        "conversion of paragraph to sentence "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS_acPE0efR2"
      },
      "source": [
        "sentence = nltk.sent_tokenize(paragraph)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee-415-lePtX"
      },
      "source": [
        "intializing the object \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENzQ6iCDeNVv"
      },
      "source": [
        "stemmer=PorterStemmer()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ4B-62NiK0W"
      },
      "source": [
        "steming \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rkXSLEChKbA"
      },
      "source": [
        "for i in range(len(sentence)):\n",
        "    words = nltk.word_tokenize(sentence[i])# Tokenization of sentence into words \n",
        "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))] # stiming the word and removing stopwords \n",
        "    sentence[i] = ' '.join(words)   # combing the stim words in a sentence \n",
        "    \n",
        "    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt4ayZB4iMBn"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}