{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qymS0C5g_0JS"
      },
      "source": [
        "# Tokenization\n",
        "# it is the process of  conversion of paragrph into sentences and words "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KED8s9eUDuIb"
      },
      "source": [
        "installing nltk library for tokenixation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cViOAjnqA_3m",
        "outputId": "ee035d90-3679-4253-ae6e-1f4da38d098f"
      },
      "source": [
        "!pip install nltk\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP24H_v2D3t7"
      },
      "source": [
        "importing the library and download all the packages through \"punk\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXwiyXWiBESM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98991b6d-8b15-4f71-a72c-38247b3ebc1a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI30OwjpEKJr"
      },
      "source": [
        "it is a paragraph for tokenization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9UOpVa9BgOc"
      },
      "source": [
        "paragragh = \"\"\"Hello, you will now learn about two major concepts of preprocessing. The first thing you'll learn about is called stemming and the second thing you will learn about is called stop words, \n",
        "and specifically you will learn how to use stemming and stop words to preprocess your texts. Let's take a look at how you can do this. Let's process this tweet. First,\n",
        " I remove all the words that don't add significant meaning to the tweets, aka stop words and punctuation marks. In practice, you would have to compare your tweet against two lists.\n",
        "  One with stop words in English and another with punctuation. These lists are usually much larger, \n",
        "  but for the purpose of this example, they will do just fine. Every word from the tweet that also appears on the list of stop words should be eliminated. \n",
        "  So you'd have to eliminate the word and, the word are, the word a, and the word at. The tweet without stop words looks like this. Note that the overall meaning of the sentence could be inferred without any effort. \n",
        "  Now, let's eliminate every punctuation mark. In this example, there are only exclamation points. The tweet without stop words and punctuation looks like this.\"\"\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-JMndoMESLa"
      },
      "source": [
        "its the tokenizatio of paragraph into sentence "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5yptrV6BnvT"
      },
      "source": [
        "sent = nltk.sent_tokenize(paragragh)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsdV1wqwEd26"
      },
      "source": [
        "tokenization of paragraph into words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdWag3wHBx48"
      },
      "source": [
        "word= nltk.word_tokenize(paragragh)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILDSPS7-FQFE"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAU7xyo4DdSr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}